{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 3 split the data into training (70%) and validation (30%)\n","**OBJECTIVE:** randomly split the annotated data into\n","- *training* (70% of the tiles): set of data used for learning (by the model), that is, to fit the parameters to the machine learning model.\n","- *validation* set (30%): Set of data used to provide an unbiased evaluation of a model fitted on the training dataset while tuning model hyperparameters.\n","Also play a role in other forms of model preparation, such as feature selection, threshold cut-off selection.\n","\n","Normally, one would create a third test dataset for a fully independent evaluation of model's performance on unseen data. In this course the test data are already taken out of the data and will be provided later in the course.\n","\n","\n","\n","**INPUT:**\n","- `path_to_tiles`=\"/content/drive/MyDrive/NOVA_course_deep_learning/data/tiles/10m_krakstad_202304_sun\"\n","-`split_train`= 0.7\n","\n","**OUTPUT:**\n","- train and validation data organized in the following folders:\n","\n","```\n","â”œâ”€â”€ train\n","â”‚   â”œâ”€â”€ images\n","â”‚   â””â”€â”€ labels\n","â”œâ”€â”€ val\n","â”‚   â”œâ”€â”€ images\n","â”‚   â””â”€â”€ labels\n","```\n"],"metadata":{"id":"vcYhX5JPvnUQ"}},{"cell_type":"code","source":["subset = \"random_subset\" # one of three model datasets (all_data, balanced_subset, random_subset) (subsets have 75% of the data)\n","\n","path = \"/content/drive/MyDrive/NOVA_deep_learning/data/\"\n","path_to_tiles = path + \"annotated_data/train/\" + str(subset)\n","\n","# define split for training and validation\n","split_train= 0.7 #\n","split_val=1-split_train"],"metadata":{"id":"ESxfoDbRtkVj","executionInfo":{"status":"ok","timestamp":1688057659948,"user_tz":-120,"elapsed":247,"user":{"displayName":"Cameron Pellett","userId":"08209286048395864181"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["### 3.1 Load libraries"],"metadata":{"id":"IOOTsKg9tf-D"}},{"cell_type":"code","source":["import os\n","import shutil\n","import random\n","\n","# mount google drive\n","# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQ9Ofi8kt8AI","executionInfo":{"status":"ok","timestamp":1688057663019,"user_tz":-120,"elapsed":1820,"user":{"displayName":"Cameron Pellett","userId":"08209286048395864181"}},"outputId":"d4bafbe3-013e-4187-9796-39421f2da4a3"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["### 3.2 Create train and validation directories and subdivide each into \"images\" and \"labels\" sub-directories"],"metadata":{"id":"6hLdXNOguHeA"}},{"cell_type":"code","source":["train_dir = os.path.join(path_to_tiles, \"train\")\n","os.makedirs(train_dir, exist_ok=True) # creates new directory for training data\n","val_dir = os.path.join(path_to_tiles, \"val\")\n","os.makedirs(val_dir, exist_ok=True) # creates new directory for validation data\n","val_img_dir = os.path.join(path_to_tiles, \"val\",\"images\")\n","os.makedirs(val_img_dir, exist_ok=True) # creates new directory for training data\n","train_img_dir = os.path.join(path_to_tiles, \"train\",\"images\")\n","os.makedirs(train_img_dir, exist_ok=True) # creates new directory for training data\n","val_ann_dir = os.path.join(path_to_tiles, \"val\",\"labels\")\n","os.makedirs(val_ann_dir, exist_ok=True) # creates new directory for training data\n","train_ann_dir = os.path.join(path_to_tiles, \"train\",\"labels\")\n","os.makedirs(train_ann_dir, exist_ok=True) # creates new directory for training data\n"],"metadata":{"id":"EO4_BRQauMrb","executionInfo":{"status":"ok","timestamp":1688057663019,"user_tz":-120,"elapsed":3,"user":{"displayName":"Cameron Pellett","userId":"08209286048395864181"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["### 3.3 Randomly sample tiles"],"metadata":{"id":"RpooLhCauWx_"}},{"cell_type":"code","source":["# Get a list of all the .txt files in the data directory\n","txt_files = [f for f in os.listdir(path_to_tiles) if f.endswith(\".txt\")]\n","img_files = [f for f in os.listdir(path_to_tiles) if f.endswith(\".tif\")]"],"metadata":{"id":"sAH2eQwTF-fQ","executionInfo":{"status":"ok","timestamp":1688057663840,"user_tz":-120,"elapsed":3,"user":{"displayName":"Cameron Pellett","userId":"08209286048395864181"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# remove .txt files that have no image (not sure why ?)\n","txt_files_with_tif = []\n","for txt_file in txt_files:\n","    # get the base name of the text file\n","    txt_base_name = os.path.basename(txt_file)\n","    # replace the file extension with .tif to get the corresponding tif file name\n","    img_file = os.path.join(os.path.dirname(txt_file), os.path.splitext(txt_base_name)[0] + '.tif')\n","    img_file=path_to_tiles+\"/\"+img_file\n","    #print(\"txt: \"+txt_file)\n","    #print(\"tif: \"+img_file)\n","    # check if the tif file exists\n","    if os.path.exists(img_file):\n","      #print(\"path to image \" + img_file + \" does not exist!\")\n","      txt_files_with_tif.append(txt_file)\n","\n"],"metadata":{"id":"l6_I5DQEDd0e","executionInfo":{"status":"ok","timestamp":1688057664486,"user_tz":-120,"elapsed":2,"user":{"displayName":"Cameron Pellett","userId":"08209286048395864181"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["txt_files=txt_files_with_tif\n","\n","# Shuffle the list of text files\n","random.shuffle(txt_files)\n","#train=random.sample(txt_files, )\n","\n","# Calculate the number of files for the train and validation sets\n","train_size = int(0.7 * len(txt_files))\n","val_size = len(txt_files) - train_size"],"metadata":{"id":"2Olsju2esPOl","executionInfo":{"status":"ok","timestamp":1688057665406,"user_tz":-120,"elapsed":2,"user":{"displayName":"Cameron Pellett","userId":"08209286048395864181"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["### Move the text annotation files and respective images to the train and validation directories"],"metadata":{"id":"DwIl8JDcyBey"}},{"cell_type":"code","source":["# iterate through each annotated .txt file\n","for i, txt_file in enumerate(txt_files):\n","    if i < train_size:\n","        dest_dir = train_dir\n","    else:\n","        dest_dir = val_dir\n","    #print(\"path to \"+path_to_tiles+\"/\"+txt_file+\" exists: \"+ str(os.path.exists(txt_file)))\n","    if os.path.exists(path_to_tiles+\"/\"+txt_file):\n","      src_file = os.path.join(path_to_tiles, txt_file)\n","      src_img = os.path.join(path_to_tiles, os.path.splitext(txt_file)[0]+\".tif\")\n","      if os.path.exists(src_img):\n","        dest_file = os.path.join(dest_dir,\"labels\", txt_file)\n","        dest_img = os.path.join(dest_dir,\"images\", os.path.splitext(txt_file)[0]+\".tif\")\n","        #print(\"copying files\")\n","        shutil.move(src_file, dest_file)\n","        shutil.move(src_img, dest_img)"],"metadata":{"id":"6sGgm68syA51","executionInfo":{"status":"ok","timestamp":1688057668350,"user_tz":-120,"elapsed":1123,"user":{"displayName":"Cameron Pellett","userId":"08209286048395864181"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["It is often also good practice to add approximately 10% of background images, i.e. that do not contain any bounding box. This will help the model to avoid to produce odd detections in areas otherwise unseen to the model.\n","\n","A simple (and efficient) way to do so it to scroll throught the image tiles (in google drive), select them manually and copying 70% of them in the training and 30% in the validation folders."],"metadata":{"id":"Cs1UJMrhyZA6"}},{"cell_type":"markdown","source":["# The end. And now let's get to the fun part ðŸ¥³ to the [model training](https://colab.research.google.com/drive/1dZ4uJHNhjbMCdk0pSyhskkA7It1jKlnF)\n"],"metadata":{"id":"l3ZaRHMmx3o7"}}]}